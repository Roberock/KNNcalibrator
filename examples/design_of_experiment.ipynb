{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T08:01:17.799003Z",
     "start_time": "2025-12-22T08:01:17.736072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from resources.OTHERS.models import paraboloid_model as simulator\n",
    "from src.forward import forward_propagation\n",
    "\n",
    "from src.plot import plot_kde_2d\n",
    "from src.backward import KNNCalibrator\n",
    "import matplotlib.pyplot as plt\n",
    "from src.plot import scatter_post\n",
    "\n",
    "from src.doe import run_sequential_doe_knn, select_next_xi_by_eig"
   ],
   "id": "3f8e042349ed460a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample_true_uncertainty_model_2d(N = 10, rng = np.random.default_rng()):\n",
    "    xa = rng.normal(4.0, 0.5, size=(N, 2))   # aleatoric\n",
    "    xe = np.tile([2.0, 0.0], (N, 1))         # epistemic (fixed, repeated)\n",
    "    return np.hstack([xa, xe])\n",
    "\n",
    "# generate data for the horse and pony show\n",
    "N_emp = 200\n",
    "theta_true_cloud = sample_true_uncertainty_model_2d(200)\n",
    "xi_list_c3 = [-2.0]\n",
    "observations_c3 = []\n",
    "for xi in xi_list_c3:\n",
    "    y_emp  = simulator(theta_true_cloud, xi)  # shape (100,1) per design\n",
    "    observations_c3.append((y_emp, xi))\n",
    "\n",
    "print(f\"CASE 3 - designs: {len(observations_c3)} samples: {observations_c3[0][0].shape[0]}\")\n"
   ],
   "id": "75b54d6f7c22e55d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class UM_theta:\n",
    "    def __init__(self,a, b, xe1 = 2.0, xe2 = 0.0):\n",
    "        self.a = 4.0 if a is None else a\n",
    "        self.b = 0.5 if b is None else b\n",
    "        self.xe1 = xe1\n",
    "        self.xe2 = xe2\n",
    "\n",
    "    def sample(self, n_samples=100):\n",
    "        xa = np.random.normal(self.a, self.b, size=(n_samples, 2))   # aleatoric\n",
    "        xe = np.tile([self.xe1, self.xe2], (n_samples, 1))         # epistemic (fixed, repeated)\n",
    "        return np.hstack([xa, xe])\n",
    "\n",
    "    def update(self,a,b, xe1, xe2):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.xe1 = xe1\n",
    "        self.xe2 = xe2\n",
    "\n",
    "class UniformPrior:\n",
    "    def __init__(self, a=-10 , b=10, xe1=2.0, xe2=0.0):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.xe1 = xe1\n",
    "        self.xe2 = xe2\n",
    "\n",
    "    def sample(self, n_samples=100):\n",
    "        xa = np.random.uniform(self.a, self.b, size=(n_samples, 2))   # aleatoric\n",
    "        xe = np.tile([self.xe1, self.xe2], (n_samples, 1))         # epistemic (fixed, repeated)\n",
    "        return np.hstack([xa, xe])\n",
    "\n",
    "    def update(self, a, b, xe1, xe2):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.xe1 = xe1\n",
    "        self.xe2 = xe2"
   ],
   "id": "b680f08d56d22a83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate simulation from prior for a design xi0\n",
    "N = 500_000\n",
    "xi0 = 1.0 # assume first design was given\n",
    "theta_uniform = UniformPrior(-20,20).sample(N)\n",
    "xi_uniform = np.random.uniform(-1,1, (N,1)) *0 + xi0\n",
    "Y_sim = simulator(theta_uniform,xi=xi_uniform)\n",
    "df_sim = pd.DataFrame(np.hstack([xi_uniform, theta_uniform, Y_sim]), columns=['xc','xa1','xa2','xe1','xe2','y' ])\n",
    "\n",
    "# generate empirical data from the unknown pdf\n",
    "pdf_theta = UM_theta(a=4.0, b=0.5)\n",
    "y, samples = forward_propagation(pdf_theta, simulator, xi=xi0, n_samples=500) # empirical data\n",
    "D_emp = {'xi': xi0, 'y': y}"
   ],
   "id": "234947b369378c43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# perform calibration get posterior using the KNN calibrator\n",
    "\n",
    "simulated_data = {\"y\": Y_sim, \"theta\": theta_uniform[:,:2], \"xi\": xi_uniform}\n",
    "\n",
    "calib_comb = KNNCalibrator(knn=50, evaluate_model=False, a_tol=0.15)  # 'prep model\n",
    "calib_comb.setup(simulated_data=simulated_data, xi_list=[1.0])\n",
    "\n",
    "obs = [(y.reshape(-1,1), xi0)]\n",
    "post_reuse = calib_comb.calibrate(obs, combine=\"stack\", resample_n=10000)\n",
    "xa_posterior = post_reuse[\"theta\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "scatter_post(ax, xa_posterior, truth=theta_true_cloud[:,:2],\n",
    "             title=r\"Posterior $p(\\theta|Y^e,\\xi)$ via kNN + in-out simulations \")\n",
    "plot_kde_2d(xa_posterior[:,:2], true_theta=theta_true_cloud[:,:2], ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# forward push to get simulated Y output for p(\\theta|Y^e,\\xi) and xi\n",
    "xe = np.tile([2.0, 0.0], (np.shape(xa_posterior)[0], 1))         # epistemic (fixed, repeated)\n",
    "theta_posterior = np.hstack([xa_posterior, xe])\n",
    "y_post_xi0, samples_posterior_xi0 = forward_propagation(theta_posterior, simulator, xi=xi0, n_samples=500) # empirical data\n",
    "\n",
    "# compare conditional prior vs conditional posterior\n",
    "plt.hist(Y_sim, density=True, label=r'prior $p(y|\\xi)$')\n",
    "plt.hist(y_post_xi0, density=True, label=r'posterior $p(y|Y^e, \\xi)$')\n",
    "plt.legend()\n",
    "plt.grid()"
   ],
   "id": "c90bac07a9e4c3f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "xi_candidates = np.linspace(-5, 5, 21)   # choose your design grid\n",
    "xi_next, eig_scores, diag = select_next_xi_by_eig(\n",
    "    simulator=simulator,\n",
    "    xa_posterior=xa_posterior,\n",
    "    xi_candidates=xi_candidates,\n",
    "    xe_fixed=(2.0, 0.0),\n",
    "    n_eval=20000,\n",
    "    k_mi=50,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "print(\"Next xi:\", xi_next)\n",
    "print(\"Diagnostics:\", diag)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xi_candidates, eig_scores, marker=\"o\", markersize=3)\n",
    "plt.axvline(xi_next, linestyle=\"--\")\n",
    "plt.title(\"EIG proxy (MI) vs xi\")\n",
    "plt.xlabel(\"xi\")\n",
    "plt.ylabel(\"I(xa ; y | xi)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "121e380662ca8864",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "xi_candidates = np.linspace(-5, 5, 41)\n",
    "\n",
    "pdf_theta_true = UM_theta(a=4.0, b=0.5)        # “real” uncertainty model\n",
    "pdf_theta_sim  = UniformPrior(a=-20, b=20)     # simulator archive prior\n",
    "\n",
    "hist = run_sequential_doe_knn(\n",
    "    simulator=simulator,\n",
    "    pdf_theta_true=pdf_theta_true,\n",
    "    pdf_theta_prior=pdf_theta_sim,\n",
    "    xi0=1.0,\n",
    "    xi_candidates=xi_candidates,\n",
    "    nq=5,\n",
    "    n_emp=100,\n",
    "    n_sim=250_000,\n",
    "    knn=50,\n",
    "    a_tol=0.05,\n",
    "    combine=\"stack\",       # strongly recommended over intersect for stability\n",
    "    resample_n=2_000,\n",
    "    n_eval_eig=5_000,\n",
    "    k_mi=30,\n",
    "    seed=0\n",
    ")\n"
   ],
   "id": "695044202fea1e3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for post_xa_i in hist['post']:\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    scatter_post(ax, post_xa_i, truth=theta_true_cloud[:,:2],\n",
    "                 title=r\"Posterior $p(\\theta|Y^e,\\xi)$ via kNN + in-out simulations \")\n",
    "    plot_kde_2d(post_xa_i[:2500,:2], true_theta=theta_true_cloud[:,:2], ax=ax)\n",
    "    ax.set_xlim([-15,15])\n",
    "    ax.set_ylim([-15,15])\n",
    "    plt.show()"
   ],
   "id": "93be8c91765778a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# quick diagnostic plots\n",
    "xi_candidates = np.asarray(np.linspace(-5,5,41), float)\n",
    "\n",
    "plt.figure()\n",
    "for q, sc in enumerate(hist[\"scores\"], start=1):\n",
    "    plt.plot(xi_candidates, sc, alpha=0.4)\n",
    "plt.scatter(hist[\"xi\"], [max(s) if i>0 else np.nan for i,s in enumerate([None]+hist[\"scores\"])],\n",
    "            s=20)\n",
    "plt.title(\"EIG proxy curves over sequential steps\")\n",
    "plt.xlabel(\"xi\")\n",
    "plt.ylabel(\"I(xa ; y | xi)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"chosen xis:\", hist[\"xi\"])\n",
    "print(\"timing:\", {k: (np.sum(v) if isinstance(v, list) else v) for k,v in hist[\"timing\"].items()})\n"
   ],
   "id": "66b9c62c11e0d063",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tasks:\n",
    "Use the available information to select new design set (set of points) $X_c = \\{ x_{c,q}, ~ q=1,...,n_q \\}$, up to a maximum experimental budget $n_q$, from which to collect new model responses to maximize uncertainty reduction in $x_e, x_a$.Each query generates a new empirical data set besides the $q=0$ which is already available.\n",
    "\n",
    "* **CASE 1** Sequential design --> Assume we can query $q=1,...,n_q$ sequentially.\n",
    "\n",
    "* **CASE 2** Non-sequential design --> Assume we select $q=1,...,n_q$ jointly and then gather data all in once.\n",
    "\n"
   ],
   "id": "f1f6dec575667ac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "eb47a9fc07828478"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optimal Design of Experiments Formulation\n",
    "\n",
    "Let $x = (x_a, x_e, x_c)$, where $x_a$ are aleatoric variables, $x_e$ epistemic parameters, and\n",
    "$x_c$ design variables. Let $y$ denote the observed system response.\n",
    "We use $p(\\cdot)$ to denote probability density functions.\n",
    "The superscript $(q)$ indicates conditioning on the design $x_{c,q}$.\n",
    "\n",
    "---\n",
    "\n",
    "### CASE 2: Non-sequential (batch) experimental design\n",
    "\n",
    "Let $X_c = \\{ x_{c,1}, \\ldots, x_{c,n_q} \\}$ be a batch of design points.\n",
    "\n",
    "#### CASE 2A: Conditional independence assumption\n",
    "\n",
    "Assume that the observations $\\{y_q\\}_{q=1}^{n_q}$ are conditionally independent given\n",
    "$x_e$ and $x_a$, i.e.\n",
    "\n",
    "$$\n",
    "p(y_{1:n_q} \\mid x_e, x_a, X_c)\n",
    "= \\prod_{q=1}^{n_q} p(y_q \\mid x_e, x_a, x_{c,q}).\n",
    "$$\n",
    "\n",
    "The batch design problem is formulated as maximization of the total expected information gain:\n",
    "\n",
    "$$\n",
    "X_c^\\star\n",
    "= \\arg\\max_{X_c}\n",
    "\\sum_{q=1}^{n_q}\n",
    "\\mathbb{E}_{p(y \\mid x_{c,q})}\n",
    "\\left[\n",
    "D_{\\mathrm{KL}}\n",
    "\\left(\n",
    "p(x_e \\mid y, x_{c,q})\n",
    "\\;\\|\\;\n",
    "p(x_e)\n",
    "\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "Equivalently, using the joint expectation form:\n",
    "\n",
    "$$\n",
    "X_c^\\star\n",
    "= \\arg\\max_{X_c}\n",
    "\\sum_{q=1}^{n_q}\n",
    "\\mathbb{E}_{p(x_e)}\n",
    "\\mathbb{E}_{p(x_a)}\n",
    "\\mathbb{E}_{p(y \\mid x_e, x_a, x_{c,q})}\n",
    "\\left[\n",
    "\\log\n",
    "\\frac{\n",
    "p(y \\mid x_e, x_{c,q})\n",
    "}{\n",
    "p(y \\mid x_{c,q})\n",
    "}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### CASE 2B: No independence assumption (joint entropy formulation)\n",
    "\n",
    "If the observations are not conditionally independent, the batch design must be optimized\n",
    "jointly by maximizing the mutual information between $x_e$ and the full observation vector\n",
    "$y_{1:n_q}$:\n",
    "\n",
    "$$\n",
    "X_c^\\star\n",
    "= \\arg\\max_{X_c}\n",
    "I(x_e ; y_{1:n_q} \\mid X_c).\n",
    "$$\n",
    "\n",
    "This can be written explicitly as:\n",
    "\n",
    "$$\n",
    "X_c^\\star\n",
    "= \\arg\\max_{X_c}\n",
    "\\mathbb{E}_{p(x_e)}\n",
    "\\mathbb{E}_{p(y_{1:n_q} \\mid x_e, X_c)}\n",
    "\\left[\n",
    "\\log\n",
    "\\frac{\n",
    "p(y_{1:n_q} \\mid x_e, X_c)\n",
    "}{\n",
    "p(y_{1:n_q} \\mid X_c)\n",
    "}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "This formulation accounts for dependencies across designs and does not decompose into\n",
    "independent contributions.\n",
    "\n",
    "---\n",
    "\n",
    "### CASE 1: Sequential experimental design\n",
    "\n",
    "Let $\\mathcal{D}^{\\mathrm{emp}}_{1:q-1}$ denote the empirical data collected up to step $q-1$.\n",
    "At iteration $q$, the next design point is selected by maximizing the one-step expected\n",
    "information gain:\n",
    "\n",
    "$$\n",
    "x_{c,q}^\\star\n",
    "=\n",
    "\\arg\\max_{x_c \\in \\mathcal{X}_c}\n",
    "\\mathbb{E}_{p(y \\mid x_c, \\mathcal{D}^{\\mathrm{emp}}_{1:q-1})}\n",
    "\\left[\n",
    "D_{\\mathrm{KL}}\n",
    "\\left(\n",
    "p(x_e \\mid y, x_c, \\mathcal{D}^{\\mathrm{emp}}_{1:q-1})\n",
    "\\;\\|\\;\n",
    "p(x_e \\mid \\mathcal{D}^{\\mathrm{emp}}_{1:q-1})\n",
    "\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "Equivalently:\n",
    "\n",
    "$$\n",
    "x_{c,q}^\\star\n",
    "=\n",
    "\\arg\\max_{x_c}\n",
    "\\mathbb{E}_{p(x_e)}\n",
    "\\mathbb{E}_{p(x_a)}\n",
    "\\mathbb{E}_{p(y \\mid x_e, x_a, x_c)}\n",
    "\\left[\n",
    "\\log\n",
    "\\frac{\n",
    "p(y \\mid x_e, x_c)\n",
    "}{\n",
    "p(y \\mid x_c)\n",
    "}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "After querying the system at $x_{c,q}^\\star$, the posterior distribution of $x_e$ is updated,\n",
    "and the procedure is repeated until the experimental budget is exhausted.\n",
    "\n"
   ],
   "id": "cc7d0db40db81bf4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
